<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Jichen Guo - Robotics & AI</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <!-- Header -->
  <header>
    <img src="assets/profile.jpeg" alt="Profile Picture" class="profile-pic">
    <h1>Jichen Guo</h1>
    <p style="text-align: center;">
      Researcher in Robotics & AI | University of Bremen & DFKI
    </p>
    <nav>
      <a href="#about">About</a>
      <a href="#projects">Projects</a>
      <a href="#videos">Videos</a>
      <a href="#publications">Publications</a>
      <a href="#contact">Contact</a>
    </nav>
  </header>

  <!-- About Section -->
  <section id="about">
    <h2>About Me</h2>
    <p>
      I am a Robotics and AI researcher specializing in Machine Learning, Computer Vision, and Robotics.
      For 4.5 years, I worked as a scientific researcher at the University of Bremenâ€™s Robotics Research Group,
      collaborating closely with the DFKI Robotics Innovation Center. My work has spanned multiple projects in robotics and AI,
      and I am now seeking new opportunities to apply my expertise and contribute to advancing the field.
    </p>
  </section>

  <!-- Projects Section -->
  <section id="projects">
    <h2>Projects</h2>
    <ul>

      <li>
        <strong><a href="https://github.com/JichenGuo/Master_Project-Mouth-State-Detection">Master Project </a></strong> â€“
        open/closed mouth state detection for robot-assisted drinking task, part of the BMBF project <a href="https://www.interaktive-technologien.de/projekte/mobile"> MobILe</a>
      </li>

      <li>
        <strong><a href="https://github.com/JichenGuo/Master_Thesis">Master Thesis </a></strong> â€“
         modification of YOLOv3 for object distance estimation, part of the EU project <a href="https://cordis.europa.eu/project/id/730836"> SMART</a>
      </li>

      <li>
        <strong><a href="https://www.biba.uni-bremen.de/aktuelles/article/kuenstliche-intelligenz-als-individueller-mentor-fuer-menschen-in-der-manuellen-produktion.html">
          Student Job at BIBA </a></strong> â€“ ergonomic analysis at manual assembly station
      </li>


      <li>
        <strong><a href="https://robotik.dfki-bremen.de/en/research/projects/insys">INSYS (Interpretable Monitoring Systems)</a></strong> â€“
        to build interpretable self-monitoring robotic systems for safe rover exploration
      </li>

      <li>
        <strong><a href="https://robotik.dfki-bremen.de/en/research/projects/physwm">PhysWM (Learning from Causal Physical World Models)</a></strong> â€“
        to reduce sample and model complexity of robot learning algorithms
      </li>

      <li>
        <strong><a href="https://robotik.dfki-bremen.de/en/research/projects/modkom">MODKOM (Modular components as Building Blocks for application-specific configurable space robots)</a></strong>
        â€“ to develop modular construction kits that allows robotic systems to be flexibly adapted to different mission scenarios
      </li>

    </ul>
  </section>

  <!-- Videos Section -->
  <section id="videos">
    <h2>Videos</h2>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/nrBLhpDkwd0"
            title="YouTube video player" frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen>
</iframe>

  </section>

  <!-- Publications Section -->
  <section id="publications">
    <h2>Publications</h2>
    <ul>
      <li>
        <a href="https://arxiv.org/abs/2407.03241">Terrain Classification Enhanced with Uncertainty for Space Exploration Robots from Proprioceptive Data</a>,
        In Journal of LatinX in AI (LXAI) Research at ICML, 2023.
      </li>

      <li>
        <a href="https://www.cs.ox.ac.uk/isg/conferences/tmp-proceedings/NeSy2023/paper24.pdf">PhysWM: Physical World Models for Robot Learning</a>,
        In NeSy 2023: 17th International Workshop on Neural-Symbolic Learning and Reasoning, (NeSy-2023), 3.7.-5.7.2023, Certosa di Pontignano, Siena, CEUR Workshop Proceedings, Jul/2023.
      </li>

      <li>
        <a href="https://arxiv.org/pdf/2409.08351?">Bayesian Inverse Graphics for Few-Shot Concept Learning</a>,
        In International Conference on Neural-Symbolic Learning and Reasoning 2024, (NeSy-2024), Springer, pages 141-165, 2024. Springer.
      </li>

      <li>
        <a href="https://arxiv.org/abs/YOUR_PAPER_ID">Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping</a>,
       Under revision in the Journal Robotics and automation Letters (RAL), 2025.
      </li>
    </ul>
  </section>

  <!-- Contact Section -->
  <section id="contact">
    <h2>Contact</h2>
    <p>
      ðŸ“§ <a href="mailto:jichen.guo [at] outlook.com">jichen.guo [at] outlook.com</a>
      ðŸ’» <a href="https://github.com/JichenGuo">GitHub</a> |
      ðŸ”— <a href="https://www.linkedin.com/in/jichen-guo-87264b14a/">LinkedIn</a>
    </p>
  </section>

  <footer>
    <p>Â© 2025 Jichen Guo | Hosted on <a href="https://pages.github.com/">GitHub Pages</a></p>
  </footer>

</body>
</html>
